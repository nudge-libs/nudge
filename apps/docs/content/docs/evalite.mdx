---
title: Testing with Evalite
description: Use Evalite with Nudge to test real prompt behavior and drive continuous self-improvement.
---

import { Callout } from 'fumadocs-ui/components/callout';

A practical guide to integrating **Nudge** with **Evalite** for testing and improving AI-powered applications.

## Why Nudge + Evalite?

Nudge and Evalite solve different parts of the same problem:

- **Nudge** defines *what the prompt should do* (structure, intent, constraints)
- **Evalite** verifies *what the prompt actually does* when run against real data

Used together, they form a tight loop:

1. **Define** prompts with Nudge
2. **Generate** system prompts with AI
3. **Evaluate** real outputs using Evalite
4. **Promote failures into tests**
5. **Auto-improve** prompts with `nudge improve`

Evalite is where you *discover* problems.  
Nudge is where you *make them impossible to reintroduce*.

---

## Setup

### Install dependencies

```bash
npm install @nudge-ai/core @nudge-ai/cli
npm install -D evalite vitest autoevals
```

### Initialize Nudge

```bash
npx @nudge-ai/cli init
```

### Add scripts

```json
{
  "scripts": {
    "nudge:generate": "nudge generate",
    "eval:dev": "evalite watch",
    "eval:ci": "evalite run"
  }
}
```

---

## Core Integration Pattern

Nudge generates prompts. Evalite executes and scores them.

Evalite evals always:

- run real LLM calls
- return string outputs
- score behavior across many inputs

Those outputs are exactly what Nudge's `.test()` API consumes.

---

## Step 1: Define the Prompt (Nudge)

```typescript
// src/summarizer.prompt.ts
import { prompt } from "@nudge-ai/core";

export const summarizerPrompt = prompt("summarizer", (p) =>
  p
    .persona("expert summarizer")
    .input("text to summarize")
    .output("concise summary")
    .do("preserve key facts", { nudge: 4 })
    .dont("add opinions", { nudge: 5 })
    .constraint("keep under 150 words")
);
```

Generate it:

```bash
npm run nudge:generate
```

---

## Step 2: Evaluate Real Behavior (Evalite)

```typescript
// src/summarizer.eval.ts
import { evalite } from "evalite";
import { Levenshtein, FactualConsistency } from "autoevals";
import { summarizerPrompt } from "./summarizer.prompt";
import "./prompts.gen";

evalite("Summarizer Quality", {
  data: [
    {
      input: "Climate change refers to long-term shifts...",
      expected: "Climate change involves long-term shifts in temperature...",
    },
  ],

  task: async (input) => {
    return callLLM({
      system: summarizerPrompt.toString(),
      user: input.input,
    });
  },

  scorers: [
    Levenshtein,
    (input, output) => ({
      name: "Factual consistency",
      score: FactualConsistency({ output, expected: input.input }),
    }),
    (input, output) => ({
      name: "Length",
      score: output.split(/\s+/).length <= 150 ? 1 : 0,
    }),
  ],
});
```

Run it:

```bash
npm run eval:dev
```

Evalite will:

- execute the prompt on all inputs
- score outputs
- show failures in a live UI (localhost:3006)
- persist results for tracking over time

---

## Step 3: Turn Evalite Failures into Nudge Tests

Evalite answers: *"What goes wrong in practice?"*

Nudge tests answer: *"This must never happen again."*

If Evalite shows failures like:

- output too long
- missing key facts
- added opinions

Encode them directly in the prompt:

```typescript
export const summarizerPrompt = prompt("summarizer", (p) =>
  p
    .persona("expert summarizer")
    .input("text to summarize")
    .output("concise summary")
    .test(
      "Climate change refers to long-term shifts...",
      (output) => output.length < 150,
      "Summary must stay under 150 words",
    )
    .test(
      "Climate change refers to long-term shifts...",
      (output) => !/should|believe|opinion/i.test(output),
      "Must not add opinions",
    )
);
```

Now these expectations are part of the prompt contract.

---

## Step 4: Self-Improve with Nudge

```bash
npx @nudge-ai/cli improve --judge
```

Nudge will:

- Detect failing tests
- Ask the LLM how to fix them
- Update the generated prompt
- Re-run tests iteratively

<Callout>
Evalite discovers the failure modes. Nudge's `improve` command fixes them at the prompt level.
</Callout>

---

## Advanced Patterns

### Testing Variants

Evalite works naturally with Nudge variants:

```typescript
summarizerPrompt.toString({ variant: "academic" });
summarizerPrompt.toString({ variant: "casual" });
```

Run separate evals per variant to compare tone, clarity, or accuracy.

### Optional Blocks

```typescript
formatterPrompt.toString({ json: true });
```

Evalite can assert JSON validity, formatting, or schema compliance.

### LLM-as-Judge

Use Evalite for nuanced scoring, then convert consistent failures into string-based Nudge tests:

```typescript
.test(
  "input",
  "should preserve all factual details without exaggeration",
)
```

These integrate cleanly with `--judge`.

---

## Best Practices

- Use Evalite to explore behavior on real data
- Promote stable findings into Nudge `.test()`
- Let `nudge improve` automate what it can
- Keep generated files disposable
- Treat `.prompt.ts` as the source of truth

---

## When to Use What

| Tool | Purpose |
|------|---------|
| Nudge `.test()` | Enforce prompt contracts |
| Evalite | Discover real-world failures |
| `improve` | Close the loop automatically |

---

## Summary

Evalite makes prompt behavior observable.  
Nudge makes it correct by construction.

Together, they turn prompt engineering into a measurable, iterative, and self-improving system.

